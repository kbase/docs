## ARAST OUTLINE ##


## System Modules ##

* Client
** arast-run
*** TODO assembler-specific commandline parameters
** arast-stat
*** DONE get all info
*** TODO get info for job id
*** TODO get specific data
** arast-get
*** get specific data
   
* Decision Engine
Based on QC preprocessing, implicit and explicit user parameters, routes jobs to specific work queues
** TODO Decide what assemblers
** DONE STARTED Write metadata
** DONE Get storage location (shock)

* Queueing Manager
Manages variable types of queues via RabbitMQ
** DONE Recieve RPC call and add job to queue
* Metadata
A MongoDB server to store all job metadata

* Volume Manager
** TODO ZFS

* Compute Node Monitor
Monitors compute instances, building and tearing down VMs as needed
** DONE arastd
Runs on compute instances.  Monitors job queues, runs pending, configuration-specific, jobs.
** DONE Startup
*** DONE Check Mongodb connection
** Consume pending job
*** TODO Keep a assembler queue (python module, or rabbitmq?)
*** DONE Run assemblies
*** DONE Update metadata for progress

## Auxillary ##
* TODO Start-up script
** DONE Init MongoDB
** TODO Init RabbitMQ


* TODO Create OpenStack VM Images
** TODO Control Server
** TODO Compute Nodes

## Example workflow ##
* Client: 'arast.py -a kiki velvet -d /home/cbun/test_data/ -l username
-> Authentication
* ???: Send back upload url / initiate transfer
* ???: Update metadata{'status':'transferring'}
-> Transfer complete
* router: post to queue
* ???: Update metadata{'status':'queued', 'data':'$DATA'}
* arastd: consume job
** start QC
* arastd: Update metadata{'status':'QC'}
-> Finished QC
* arastd: Update metadata{'status':'QC done','qc_data':'$QC_FILE'}
* arastd: Start assemblies
* arastd: Update metadata{'status':'Assembly...'}



## Database Collections ##
* Jobs
** date_submitted
** date_completed
** id
** status
** data
** qc_data

* Quality
** binary/file
